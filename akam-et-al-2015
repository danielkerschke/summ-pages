Akam, T., Costa, R., & Dayan, P. (2015). Simple plans or sophisticated habits? State, transition and learning interactions in the two-step task. PLOS Computational Biology, 11(12), e1004648. https://doi.org/10.1371/journal.pcbi.1004648.

The paper asks a simple question: when we see “model-based” looking choices in the two-step task, are they really the product of planning, or can simpler learned habits produce the same pattern. The authors show that the task’s structure can let purely model-free learners look model-based, and they map out when and why this happens. They also propose analysis checks and task tweaks to tell planning from habit more reliably.
-- In the “two-step” sequential decision task, you pick between two first-stage actions. Each action usually leads to one particular second-stage state with a “common” transition, about 70 percent of the time, and occasionally to the other second-stage state with a “rare” transition, about 30 percent of the time. At the second stage you make another choice and receive reward with probabilities that drift slowly over time. A purely model-free learner tends to repeat first-stage actions that were rewarded on the previous trial, regardless of whether the intervening transition was common or rare. A model-based learner uses the learned transition structure, so it tends to repeat after a rewarded common transition but switch after a rewarded rare transition, because getting a reward through a rare transition suggests the other first-stage action will more reliably reach the profitable second-stage state next time. Human behavior shows both patterns, and fMRI reveals that ventral striatum carries signals consistent with a blend of model-free and model-based prediction errors that best explains the subject-by-subject mixture in choice. This is the core evidence that a canonical dopamine target reflects computations with model-based content, even in a task designed to reveal habit. (Daw et al., 2011).

The first key point is an analysis pitfall. Many papers look at “stay” behavior on the next trial after a reward with a common or rare transition. In theory, a model-free learner should only care about reward, not the transition, while a model-based learner should show a reward by transition interaction. Akam and colleagues show that even a standard model-free agent can show this interaction, because the values at the start of a trial co-vary with which transition-outcome combination is likely to be observed next. In plain words, when you were already leaning toward the correct first-step choice, you were also more likely to see a “rewarded-common” or “unrewarded-rare” event, which inflates the stay interaction. A simple fix helps: add a “was the previous choice correct” regressor to the logistic regression, or use a lagged analysis that looks beyond a single trial, which removes the spurious interaction for model-free agents while preserving the true model-based signature.

The second key point is about alternative strategies that mimic planning. Because rewards tend to come from one second-step state more than the other within a block, an agent can learn a shortcut. One shortcut is “reward-as-cue”: if I was rewarded in state a, pick the first-step action that usually leads to a; if I was rewarded in state b, pick the one that leads to b. A more powerful shortcut is a “latent-state” strategy: infer which hidden block you are in, the a-good block or the b-good block, by integrating recent reward locations, then use a fixed mapping from that inferred state to the first-step choice. Both strategies can look model-based under classic stay analyses, and the latent-state strategy can even match model-based performance, yet neither performs prospective evaluation. The authors recommend guarding against these look-alikes by using the corrected regressions, by fitting and comparing explicit models including latent-state ones, and by considering task variants that sometimes flip the first-step-to-second-step transition mapping, which breaks the simple reward-as-cue link.
