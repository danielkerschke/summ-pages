Gershman, S. J. (2015). A unifying probabilistic view of associative learning. PLOS Computational Biology, 11(11), e1004567. https://doi.org/10.1371/journal.pcbi.1004567

The paper starts from two simple ideas about how animals learn from experience. First, they keep track of how sure they are about what causes what, not just the average strength of a link. That is the Bayesian idea of carrying a distribution over possibilities rather than a single guess. Second, they care about the chain of rewards that will arrive in the future, not only the reward that occurs right now. That is the reinforcement learning idea of learning values that predict long-term return. The classic Rescorlaâ€“Wagner rule can be seen as a rough point-estimate version of these ideas. Gershman shows how to place it in a probabilistic frame so that uncertainty and long-term prediction become explicit parts of the story. 

To make uncertainty concrete, the paper uses the Kalman filter. You can think of it as a recipe that updates both a best guess and a confidence band around that guess each time a cue and an outcome are observed. When a cue has been seen many times without surprise, the model becomes confident and learns more slowly from that cue. This explains latent inhibition in a very direct way. The same covariance bookkeeping also explains retrospective effects such as backward blocking and recovery from overshadowing, because learning about one cue changes beliefs about a partner cue that was paired with it earlier. The key is that the model does not forget the relationships among cues while it updates them. 

To bring in time and future reward, the paper combines the Kalman filter with temporal-difference learning, creating what it calls Kalman TD. Here the model still tracks uncertainty, but it learns about the value that unfolds over time within a trial, so it can handle serial compounds, gaps, and second-order conditioning. This unified model captures cases that neither approach can handle alone, like how extinguishing one element of a trained sequence can change responding to another element that is absent at test. The broader message is that associative learning looks like tracking a changing distribution over values through time, which links animal learning, probabilistic inference, and reinforcement learning in one picture. 
