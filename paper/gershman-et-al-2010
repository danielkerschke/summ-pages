Gershman, S. J., & Niv, Y. (2010). Learning latent structure: Carving nature at its joints. Current Opinion in Neurobiology, 20(2), 251–256. https://doi.org/10.1016/j.conb.2010.02.008

Start from the core problem. Animals and people learn from trial and error, which the field calls reinforcement learning. In a toy world with only a few situations and choices, simple learning rules work fine. In the real world with many sights, sounds, and actions, those same rules crawl, because they try to learn about everything at once. The key idea in this paper is that learners do something smarter. They look for hidden structure in the world that groups things together, and they only learn about the parts that matter for getting rewards and avoiding punishments. In other words, the brain tries to infer the unseen causes that generate what we observe, then it treats those inferred causes as the meaningful “states” and “options” to learn about. This trims the problem down to size and speeds learning dramatically. 

Here is an intuitive example. In fear conditioning, a tone is followed by a shock during training, but during extinction the tone comes without shock. If the animal simply averaged across all trials, it would predict “shock half the time,” which is not what we see. Instead, the animal behaves as if it infers two hidden situations: a training situation and an extinction situation. When the context looks like training, it expects shock after the tone. When the context looks like extinction, it expects safety. This way of thinking explains renewal, reinstatement, and spontaneous recovery, because changes in place or time act like cues that shift the animal’s guess about which hidden situation is active. The authors cast this as Bayesian structure learning in perception. The learner uses experience to update beliefs about which latent cause is generating the tone and outcome right now, and it learns only within the relevant cause, rather than mixing everything together. 

---

**The same strategy helps with actions.** 
Think about moving as choosing with parts. Each part of your body that can act is an effector. A hand is an effector. An eye movement is an effector. If you tried to learn the value of every possible combination of all parts at once, the number of choices would blow up. Two hands with three options each already give nine combinations. Add fingers, wrist angles, and timing, and the count becomes huge. Trying to learn a separate “goodness” number for each combo would take forever.

So the brain cheats in a smart way. It breaks the big choice into smaller choices that line up with the parts. Instead of storing a value for “left hand grip tight while right hand loosens,” it keeps a value for “left hand grip tight” and a value for “right hand loosen,” then adds them when both happen together. You can feel this when you learn piano or typing. You practice small chunks with one hand, then the other, and later you put them together. By reusing part values, you learn faster, because each practice trial updates something that will help in many future combinations.

Different brain systems seem to support this trick. The hippocampus helps slice the situation into the right pieces, like “this context calls for these chunks.” The prefrontal cortex keeps track of the current rule or plan, such as “left hand leads, right hand follows.” The basal ganglia and dopamine system act like a coach with a scoreboard. When an action makes things better or worse than expected, dopamine carries that surprise to update the values for the relevant parts. Because each hemisphere mainly controls the opposite side of the body, you often see learning signals split by hemisphere in brain data, which fits the idea of part specific values. Put simply, complex action becomes manageable because the brain learns reusable pieces and then composes them.
