Gershman, S. J. (2025). Bridging computation and representation in associative learning. Computational Brain & Behavior, 8, 377–391. https://doi.org/10.1007/s42113-025-00242-y

In Pavlovian conditioning, a cue like a tone predicts a reward like food. Classic “associative” theories say the animal learns a direct link between the cue and the reward based on how close in time they appear, and the strength of that link drives behavior. A rival “representational” view, Rate Estimation Theory, says the animal instead keeps track of how often rewards arrive during the cue versus in the background, and it responds when the cue’s rate is high compared to the background rate. This shift from contiguity to contingency explains findings like timescale invariance, where stretching both cue–reward and between-cue intervals by the same factor leaves learning speed unchanged, and it explains why adding background rewards can weaken responding even if cue–reward pairings are unchanged. 
-- Think of the animal as asking a very simple question each moment: “If the cue is on right now, how much sooner should I expect the next reward compared with ordinary time when the cue is off?” If you stretch all the waiting times by the same factor, that comparison barely changes, because both the “during the cue” rate and the “background” rate slow down together, so the ratio stays about the same. The cue is no more or less informative, so learning and responding look just as fast in relative time. Now add extra background rewards (opportunity cost). Those make good things happen even when the cue is absent, which shrinks the gap between “with cue” and “without cue.” The cue no longer tells you very much about when the next reward will come, so it loses some pull. It's no longer as big of an improvement.

Gershman shows that Rate Estimation Theory, as usually formulated, is hard to compute because it demands heavy bookkeeping about which stimuli co-occur. He then gives a simpler fix. You can estimate those rates in continuous time with a very lightweight, error-driven update that looks almost exactly like the famous Rescorla–Wagner learning rule, but interpreted as estimating reinforcement rates rather than “associative strength.” The crucial difference lies not in how you learn, but in how you decide to act. The response rule compares the cue’s estimated rate plus background to the background alone, which can be read as an information gain about when the next reward will arrive. With a fixed threshold on this information gain, the model recovers timescale invariance and other key effects, and even yields a simple law: reinforcements to acquisition scale with the inverse of informativeness, roughly proportional to (cycle time divided by cue–reward delay) to the power of minus one. 
-- Intuitively, think of the animal as keeping two simple running meters for time: how fast rewards arrive in general, and how fast they arrive when a particular cue is on. The learning rule just nudges each meter up or down whenever reality is better or worse than expected, which is why it looks like Rescorla–Wagner. The choice rule then asks a very plain question: “Does the cue make the next reward feel closer in time than usual?” If yes, respond. Because this comparison uses a ratio of “during cue” to “background,” stretching all intervals keeps the ratio the same, so behavior scales cleanly across timescales. And the fewer bits of timing information each cue pairing provides, the more pairings you need before the cue feels useful, which is why trials to learn grow roughly with cycle time divided by the cue–reward delay.

Why this matters is that it bridges two camps. You can keep the efficient, biologically plausible machinery of error-driven learning, which dovetails with dopamine prediction error signals, while adopting a decision rule that respects contingency and continuous time. The paper forecasts clear tests, like how prior expectations about reward rates should speed up or slow down apparent learning without breaking the invariance law. In short, associative-style updates can build the right internal numbers, and a relative, information-based response rule turns those numbers into behavior that matches the data. 
