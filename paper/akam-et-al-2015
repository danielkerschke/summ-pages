Akam, T., Costa, R., & Dayan, P. (2015). Simple plans or sophisticated habits? State, transition and learning interactions in the two-step task. PLOS Computational Biology, 11(12), e1004648. https://doi.org/10.1371/journal.pcbi.1004648.

The paper asks a simple question: when we see “model-based” looking choices in the two-step task, are they really the product of planning, or can simpler learned habits produce the same pattern. The authors show that the task’s structure can let purely model-free learners look model-based, and they map out when and why this happens. They also propose analysis checks and task tweaks to tell planning from habit more reliably.

The first key point is an analysis pitfall. Many papers look at “stay” behavior on the next trial after a reward with a common or rare transition. In theory, a model-free learner should only care about reward, not the transition, while a model-based learner should show a reward by transition interaction. Akam and colleagues show that even a standard model-free agent can show this interaction, because the values at the start of a trial co-vary with which transition-outcome combination is likely to be observed next. In plain words, when you were already leaning toward the correct first-step choice, you were also more likely to see a “rewarded-common” or “unrewarded-rare” event, which inflates the stay interaction. A simple fix helps: add a “was the previous choice correct” regressor to the logistic regression, or use a lagged analysis that looks beyond a single trial, which removes the spurious interaction for model-free agents while preserving the true model-based signature.

The second key point is about alternative strategies that mimic planning. Because rewards tend to come from one second-step state more than the other within a block, an agent can learn a shortcut. One shortcut is “reward-as-cue”: if I was rewarded in state a, pick the first-step action that usually leads to a; if I was rewarded in state b, pick the one that leads to b. A more powerful shortcut is a “latent-state” strategy: infer which hidden block you are in, the a-good block or the b-good block, by integrating recent reward locations, then use a fixed mapping from that inferred state to the first-step choice. Both strategies can look model-based under classic stay analyses, and the latent-state strategy can even match model-based performance, yet neither performs prospective evaluation. The authors recommend guarding against these look-alikes by using the corrected regressions, by fitting and comparing explicit models including latent-state ones, and by considering task variants that sometimes flip the first-step-to-second-step transition mapping, which breaks the simple reward-as-cue link. 
