Daw, N. D., & Tobler, P. N. (2014). Value learning through reinforcement: The basics of dopamine and reinforcement learning. In P. W. Glimcher & E. Fehr (Eds.), Neuroeconomics: Decision making and the brain (2nd ed., pp. 283-298). Academic Press. https://doi.org/10.1016/B978-0-12-416008-8.00015-2

Imagine you're training a dog to expect food when it hears a bell, like in famous experiments by Pavlov. This chapter explains how our brains learn to predict rewards (like food or money) through a process called reinforcement learning. It's like trial and error: you try something, get a reward or not, and your brain adjusts to make better guesses next time. The key player here is dopamine, a chemical in the brain that acts like a teacher, signaling when your predictions are right or wrong.

Dopamine comes from special neurons in the midbrain, and it flows to areas like the striatum, which helps with movement and decisions. Think of dopamine as a messenger that broadcasts news about rewards to the rest of the brain. When something good happens unexpectedly, dopamine levels spike, making you feel motivated. But if it's expected, there's no big reaction. This helps explain why surprises can be exciting or disappointing.

The chapter starts with simple learning rules, like the Rescorla-Wagner model. It's like this: your brain keeps a running score of how much reward a thing (like a bell) usually brings. If you get more reward than expected, that's a "positive prediction error," and your brain updates the score upward. If less, it's a negative error, and the score drops. This is how habits form—your brain is constantly fine-tuning expectations based on surprises. Picture a graph where rewards from the past influence your current guess, but recent ones matter more, fading out like an echo. That's how the model works mathematically. It explains why if a bell always predicts food perfectly, adding a light at the same time doesn't teach the dog much new—the bell already "blocks" the learning because there's no surprise left.

Dopamine neurons act just like these prediction errors in real life. Experiments with monkeys show that when they get an unexpected juice reward, dopamine fires up. But if it's fully expected, nothing happens. If reward is missing when expected, dopamine dips below normal, like a "oops" signal. This matches the math model perfectly, suggesting dopamine is the brain's way of calculating these errors.

The brain's dopamine system is wired for this: neurons in areas like the substantia nigra send signals far and wide, almost like a global alert system. They're connected in loops with the cortex and striatum, influencing both movement (like in Parkinson's disease, where low dopamine causes stiffness) and motivation (why drugs like cocaine, which boost dopamine, are addictive).
But simple models have limits—they treat each trial as separate, ignoring time within a trial. Real life is sequential: a bell leads to food after a delay. That's where temporal difference learning comes in. It's an upgrade that breaks time into steps, predicting rewards moment by moment, like a chain of expectations.

In this advanced model, a cue (like a bell) can trigger dopamine if it predicts future reward, even before the reward arrives. It's like getting excited about a vacation when you book the ticket, not just when you arrive. This explains "second-order conditioning," where a new sound predicts the bell, and you learn to react to it too, building predictions in layers.

Dopamine's role ties into bigger ideas: it's not just about pleasure, but about learning to choose actions that lead to rewards. In decision-making, these signals help reinforce good choices, like picking a fruit that's usually ripe. Problems here might explain addictions or disorders where motivation goes wrong.

Overall, the chapter shows how math models from computer science match brain science, with dopamine as the bridge. It's a foundation for understanding why we learn from rewards, and it sets up more complex topics like how we make choices in uncertain worlds.
