Gershman, S. J., Assad, J. A., Datta, S. R., Linderman, S. W., Sabatini, B. L., Uchida, N., & Wilbrecht, L. (2024). Explaining dopamine through prediction errors and beyond. Nature Neuroscience. https://doi.org/10.1038/s41593-024-01705-4

The paper starts from the classic idea that short bursts of dopamine report reward prediction errors, which are the moment-to-moment mismatches between what an animal expected and what actually happened. The authors ask why this simple picture struggles with three recurring observations. First, dopamine often ramps up as an animal approaches a reward. Second, dopamine responds to sensory details, not only to reward size or timing. Third, dopamine activity can bias which actions get chosen. They show how tweaks that are motivated by the computational problem can reconcile much of this. For ramps, the key is uncertainty about where in time or space the animal currently is. When sensory feedback sharpens that internal estimate, value is overestimated and must be corrected, which leaves behind a residual ramping error signal during navigation tasks. In settings without ongoing sensory updates, like many Pavlovian tasks, that ramp vanishes. This account turns an apparent contradiction into a prediction about when ramps should or should not appear. 
-- Picture an animal walking to a treat, a bit like you walking to a bakery with a foggy sense of how far it is. Your brain keeps a running guess about “where am I now” and “how close is the reward.” That guess is noisy. Each time a new landmark pops into view, the brain briefly gets overconfident and thinks the reward is closer than it really is. That optimistic jump has to be corrected, and the correction shows up as a little burst that nudges the estimate back down. String many of these tiny corrections together while the animal moves and you see a smooth ramp that grows as the goal approaches. In tasks where there are no fresh landmarks arriving over time, like a simple tone followed by a reward, there are no stepwise corrections to stack up, so the ramp does not appear.
-- **Why overconfident?** Because value rises more sharply as you get closer to a reward, uncertainty creates a tilt toward optimism. When the animal does not know its exact place, the brain averages over “maybe I am here, maybe a bit closer, maybe a bit farther.” The possibilities that are a bit closer carry much higher value than the ones that are a bit farther, so the average skews upward. That upward skew feels like “I am closer than I probably am,” which is what we called overconfidence. Then a new landmark tightens the position estimate, removes the skew, and the system has to correct downward. Repeating this cycle as new cues arrive produces the series of small corrections that look like a ramp while moving toward the goal.

The sensory story widens the role of dopamine beyond a single scalar error about reward. Animals learn predictions about features of upcoming events, such as flavor, and dopamine transients help stitch those feature predictions together. One way to formalize this is to treat dopamine as carrying a vector of prediction errors over many features, which fits results where individual neurons are mixed in their tuning yet ensembles carry decodable information about identity changes early after a switch. The successor representation offers a bridge, since it learns a predictive map of what states or features will occur soon and then combines that map with current goals to compute value. This perspective helps explain novelty responses, identity unblocking, and the fact that some dopamine projections emphasize different kinds of features in different targets. The diversity can come from labeled lines that project to distinct striatal subregions, from distributed population codes that must be demixed downstream, or both. 

Action and motivation are handled by linking fast phasic errors to slower tonic levels that track average reward. Phasic bursts teach values and policies, while the slow baseline sets how much effort is worth spending given the going reward rate, which fits vigor and effort findings as well as effects of dopamine depletion. Some data also point to action prediction errors that register how surprising a chosen movement was, independent of reward, which can guide stable choice biases. Finally, the authors review alternative proposals that capture effects outside a pure error framework, including signals for perceived saliency, causal inference about which cues matter, and adaptive learning rates. Their conclusion is pragmatic. A unifying theory will likely treat dopamine as a family of prediction error signals, shaped by state uncertainty, feature predictions, and projection-specific circuitry, with additional roles for salience and learning rate control where the data demand it. 
