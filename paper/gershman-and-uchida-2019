Gershman, S. J., & Uchida, N. (2019). Believing in dopamine. Nature Reviews Neuroscience, 20, 703–714. https://doi.org/10.1038/s41583-019-0220-7

The paper argues that dopamine is not only a “surprise about reward” signal, often called a reward prediction error, but also a window into how the brain handles uncertainty. In reinforcement learning, three ingredients do most of the work: state, value, and policy. State is your best guess about “where you are” in a task, value is how much future reward you expect from that state, and policy is the rule that picks actions. The authors’ core idea is simple to state and rich in consequences. Each ingredient can be uncertain in different ways, and those different uncertainties feed into different parts of the cortex–basal ganglia circuit and show up in dopamine activity. This unifies classic prediction error theory with newer Bayesian ideas, which treat beliefs as graded probabilities instead of all-or-nothing facts. 

First comes state uncertainty. Animals rarely know the true state directly, since senses are noisy and clues are ambiguous. The right move is to keep a “belief state,” which is a probability distribution over possible states given the evidence so far. The medial prefrontal cortex appears to help compute and update this belief state. The striatum then encodes a compact representation of that belief and maps it to predicted value. Dopamine neurons compare what actually happens with that belief-weighted value to produce a prediction error. This explains otherwise puzzling findings, like positive dopamine responses to unrewarded cues when the animal is still unsure which cue just occurred, and the absence of a negative dip at an expected reward time when uncertainty implies the trial has likely transitioned. Inactivating medial prefrontal cortex selectively disrupts these belief-dependent patterns, which supports the proposed division of labor. 

Next comes value uncertainty, which is uncertainty about the parameters that link states to predicted rewards. The paper highlights a Bayesian version of temporal-difference learning, often called Kalman TD. In this view, the brain not only updates expected values from errors, it also tracks confidence in those values, and it adjusts learning rates accordingly. This helps explain “retrospective revaluation” effects that standard models struggle with, such as backward blocking and sensory preconditioning, where learning about one cue changes what you believe about another cue that is currently absent. It also offers an account of latent inhibition, where pre-exposing a neutral cue slows later learning because the system has grown confident that the cue predicts nothing. Orbitofrontal cortex is proposed to implement the necessary stimulus transformations that make these Bayesian updates neurally plausible. 

Finally comes policy uncertainty, which is about how confidently the system should choose one action over another. The authors connect this to two exploration styles seen in humans. Directed exploration adds an “uncertainty bonus” that biases you toward options you know less about. Random exploration increases the noisiness of choices when everything is uncertain. Genetic and behavioral evidence suggests a rough split in control, with prefrontal dopamine linked more to directed exploration and striatal dopamine linked more to random exploration. The paper also describes an alternative lens called active inference, where dopamine may report the precision, that is the confidence, of the current policy. Higher precision means crisper, more deterministic actions. Lower precision means more exploration. These accounts are not mutually exclusive, and both aim to capture how uncertainty shapes choice. 

The big picture is that uncertainty is not a nuisance the brain ignores, it is the very material dopamine helps the system work with. State uncertainty is handled by belief states that shape prediction errors. Value uncertainty tunes learning itself by scaling how strongly errors should change your beliefs. Policy uncertainty regulates how boldly or tentatively you act when you are not yet sure. The framework ties diverse dopamine findings into one story, but it also flags open problems, such as how the brain learns the structure of a task in the first place and how tonic dopamine relates to average reward versus precision. The payoff for readers is a cleaner map from computational principles to neural machinery, and a set of testable predictions that already match a surprising amount of data. 
